{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmQ/qk+NXxMJUjHzDG6sAa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "53ededcb128a42cd9c8dd02ed67385a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a206fcef7de74de28207b0ed8ff70685",
              "IPY_MODEL_6fc0e415bb04436aa49af026e859d18d",
              "IPY_MODEL_3028cabdd9fd4258a956620b79613c7e"
            ],
            "layout": "IPY_MODEL_2fab19445ef94af799ef6ad0eb67a805"
          }
        },
        "a206fcef7de74de28207b0ed8ff70685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c94356b7de1a4cdaaebe4133751e879a",
            "placeholder": "​",
            "style": "IPY_MODEL_33a3cf79a0a544be85798777ce8edc5e",
            "value": "Batches: 100%"
          }
        },
        "6fc0e415bb04436aa49af026e859d18d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a746c9e41494113b3d53d86d950d8c9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_517ed125753142e8a8e1b316712d8393",
            "value": 1
          }
        },
        "3028cabdd9fd4258a956620b79613c7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c21c192e55914adbb8e7216a9c99b836",
            "placeholder": "​",
            "style": "IPY_MODEL_ceca7e8853aa446ca79441e4f014239e",
            "value": " 1/1 [00:00&lt;00:00,  1.01it/s]"
          }
        },
        "2fab19445ef94af799ef6ad0eb67a805": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c94356b7de1a4cdaaebe4133751e879a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33a3cf79a0a544be85798777ce8edc5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a746c9e41494113b3d53d86d950d8c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "517ed125753142e8a8e1b316712d8393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c21c192e55914adbb8e7216a9c99b836": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceca7e8853aa446ca79441e4f014239e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jminangods/analisis-data/blob/main/RAG_Completo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSJgY-yrVSYX",
        "outputId": "b16cd106-8c65-48e5-af39-fe70bad59172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.7/72.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.7/251.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PASO 1: WEB SCRAPER ROBUSTO PARA MÚLTIPLES PÁGINAS\n",
        "# ============================================================================\n",
        "\n",
        "# Instalaciones para Colab\n",
        "!pip install -q requests beautifulsoup4 fpdf2 sentence-transformers faiss-cpu torch\n",
        "!pip install -q lxml html5lib  # Parsers adicionales para BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from fpdf import FPDF\n",
        "import time\n",
        "import re\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from typing import List, Dict, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "zmX6O5FocUcd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WebScraperToPDF:\n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        })\n",
        "        self.scraped_content = []\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Limpiar texto extraído\"\"\"\n",
        "        # Remover espacios extra y saltos de línea\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        # Remover caracteres especiales problemáticos\n",
        "        text = re.sub(r'[^\\w\\s\\.\\,\\;\\:\\!\\?\\-\\(\\)áéíóúñÁÉÍÓÚÑ]', '', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def scrape_single_page(self, url: str, max_length: int = 5000) -> Dict:\n",
        "        \"\"\"Scrapear una página individual\"\"\"\n",
        "        print(f\"🌐 Scrapeando: {url}\")\n",
        "\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            response.encoding = 'utf-8'\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Remover elementos no deseados\n",
        "            for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):\n",
        "                element.decompose()\n",
        "\n",
        "            # Extraer título\n",
        "            title = soup.find('title')\n",
        "            title = title.get_text().strip() if title else f\"Página de {urlparse(url).netloc}\"\n",
        "\n",
        "            # Extraer contenido principal\n",
        "            # Buscar contenedores comunes de contenido\n",
        "            content_selectors = [\n",
        "                'main', 'article', '.content', '.post', '.entry-content',\n",
        "                '[role=\"main\"]', '.main-content', '#content', '.page-content'\n",
        "            ]\n",
        "\n",
        "            content_element = None\n",
        "            for selector in content_selectors:\n",
        "                content_element = soup.select_one(selector)\n",
        "                if content_element:\n",
        "                    break\n",
        "\n",
        "            # Si no encuentra contenedor específico, usar body\n",
        "            if not content_element:\n",
        "                content_element = soup.find('body')\n",
        "\n",
        "            if content_element:\n",
        "                # Extraer texto de párrafos, headers, listas\n",
        "                text_elements = content_element.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'div'])\n",
        "\n",
        "                content_parts = []\n",
        "                for element in text_elements:\n",
        "                    text = element.get_text().strip()\n",
        "                    if len(text) > 20:  # Solo textos significativos\n",
        "                        content_parts.append(text)\n",
        "\n",
        "                content = '\\n\\n'.join(content_parts)\n",
        "            else:\n",
        "                content = soup.get_text()\n",
        "\n",
        "            # Limpiar y truncar contenido\n",
        "            content = self.clean_text(content)\n",
        "            if len(content) > max_length:\n",
        "                content = content[:max_length] + \"...\"\n",
        "\n",
        "            print(f\"✅ Extraído: {len(content)} caracteres\")\n",
        "\n",
        "            return {\n",
        "                'url': url,\n",
        "                'title': title,\n",
        "                'content': content,\n",
        "                'success': True,\n",
        "                'length': len(content)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error en {url}: {str(e)}\")\n",
        "            return {\n",
        "                'url': url,\n",
        "                'title': f\"Error: {urlparse(url).netloc}\",\n",
        "                'content': f\"No se pudo extraer contenido de {url}. Error: {str(e)}\",\n",
        "                'success': False,\n",
        "                'length': 0\n",
        "            }\n",
        "\n",
        "    def scrape_multiple_pages(self, urls: List[str], delay: float = 1.0) -> List[Dict]:\n",
        "        \"\"\"Scrapear múltiples páginas con delay\"\"\"\n",
        "        print(f\"🚀 Iniciando scraping de {len(urls)} páginas...\")\n",
        "\n",
        "        results = []\n",
        "        for i, url in enumerate(urls, 1):\n",
        "            print(f\"\\n📄 Página {i}/{len(urls)}\")\n",
        "\n",
        "            result = self.scrape_single_page(url)\n",
        "            results.append(result)\n",
        "            self.scraped_content.append(result)\n",
        "\n",
        "            # Delay entre requests para ser respetuoso\n",
        "            if i < len(urls):\n",
        "                print(f\"⏳ Esperando {delay}s...\")\n",
        "                time.sleep(delay)\n",
        "\n",
        "        successful = sum(1 for r in results if r['success'])\n",
        "        total_chars = sum(r['length'] for r in results)\n",
        "\n",
        "        print(f\"\\n✅ Scraping completado!\")\n",
        "        print(f\"   📊 Páginas exitosas: {successful}/{len(urls)}\")\n",
        "        print(f\"   📝 Total de caracteres: {total_chars:,}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def create_pdf(self, output_filename: str = \"scraped_content.pdf\") -> str:\n",
        "        \"\"\"Crear PDF con todo el contenido scrapeado\"\"\"\n",
        "        print(f\"📄 Creando PDF: {output_filename}\")\n",
        "\n",
        "        if not self.scraped_content:\n",
        "            print(\"❌ No hay contenido para crear PDF\")\n",
        "            return None\n",
        "\n",
        "        # Configurar PDF\n",
        "        pdf = FPDF()\n",
        "        pdf.set_auto_page_break(auto=True, margin=15)\n",
        "\n",
        "        # Página de título\n",
        "        pdf.add_page()\n",
        "        pdf.set_font('Arial', 'B', 16)\n",
        "        pdf.cell(0, 10, 'Contenido Web Consolidado', 0, 1, 'C')\n",
        "        pdf.set_font('Arial', '', 10)\n",
        "        pdf.cell(0, 10, f'Generado automáticamente - {len(self.scraped_content)} páginas', 0, 1, 'C')\n",
        "        pdf.ln(10)\n",
        "\n",
        "        # Índice\n",
        "        pdf.set_font('Arial', 'B', 14)\n",
        "        pdf.cell(0, 10, 'Índice', 0, 1, 'L')\n",
        "        pdf.set_font('Arial', '', 10)\n",
        "\n",
        "        for i, page_data in enumerate(self.scraped_content, 1):\n",
        "            title = page_data['title'][:60] + \"...\" if len(page_data['title']) > 60 else page_data['title']\n",
        "            try:\n",
        "                pdf.cell(0, 6, f\"{i}. {title}\", 0, 1, 'L')\n",
        "            except:\n",
        "                pdf.cell(0, 6, f\"{i}. [Título con caracteres especiales]\", 0, 1, 'L')\n",
        "\n",
        "        # Contenido de cada página\n",
        "        for i, page_data in enumerate(self.scraped_content, 1):\n",
        "            pdf.add_page()\n",
        "\n",
        "            # Título de la sección\n",
        "            pdf.set_font('Arial', 'B', 14)\n",
        "            title = page_data['title']\n",
        "            try:\n",
        "                pdf.cell(0, 10, f\"{i}. {title}\", 0, 1, 'L')\n",
        "            except:\n",
        "                pdf.cell(0, 10, f\"{i}. [Título con caracteres especiales]\", 0, 1, 'L')\n",
        "\n",
        "            # URL\n",
        "            pdf.set_font('Arial', 'I', 8)\n",
        "            pdf.cell(0, 5, f\"Fuente: {page_data['url']}\", 0, 1, 'L')\n",
        "            pdf.ln(5)\n",
        "\n",
        "            # Contenido\n",
        "            pdf.set_font('Arial', '', 10)\n",
        "            content = page_data['content']\n",
        "\n",
        "            # Dividir contenido en líneas manejables\n",
        "            try:\n",
        "                # Intentar escribir contenido normal\n",
        "                lines = content.split('\\n')\n",
        "                for line in lines:\n",
        "                    if line.strip():\n",
        "                        # Manejar líneas largas\n",
        "                        words = line.split(' ')\n",
        "                        current_line = \"\"\n",
        "                        for word in words:\n",
        "                            if len(current_line + word) < 80:\n",
        "                                current_line += word + \" \"\n",
        "                            else:\n",
        "                                if current_line:\n",
        "                                    try:\n",
        "                                        pdf.cell(0, 5, current_line.strip(), 0, 1, 'L')\n",
        "                                    except:\n",
        "                                        pdf.cell(0, 5, \"[Línea con caracteres especiales]\", 0, 1, 'L')\n",
        "                                current_line = word + \" \"\n",
        "                        if current_line:\n",
        "                            try:\n",
        "                                pdf.cell(0, 5, current_line.strip(), 0, 1, 'L')\n",
        "                            except:\n",
        "                                pdf.cell(0, 5, \"[Línea con caracteres especiales]\", 0, 1, 'L')\n",
        "                    pdf.ln(2)\n",
        "            except Exception as e:\n",
        "                pdf.cell(0, 5, f\"[Error mostrando contenido: {str(e)}]\", 0, 1, 'L')\n",
        "\n",
        "            pdf.ln(10)\n",
        "\n",
        "        # Guardar PDF\n",
        "        try:\n",
        "            pdf.output(output_filename)\n",
        "            print(f\"✅ PDF creado exitosamente: {output_filename}\")\n",
        "            return output_filename\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error creando PDF: {e}\")\n",
        "            return None\n"
      ],
      "metadata": {
        "id": "vJN59t0lbkNZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FUNCIÓN HELPER PARA URLS FÁCILES\n",
        "# ============================================================================\n",
        "\n",
        "def quick_scrape_to_pdf(urls: List[str], filename: str = \"content.pdf\", delay: float = 1.0) -> str:\n",
        "    \"\"\"Función rápida para scrapear URLs y crear PDF\"\"\"\n",
        "    scraper = WebScraperToPDF()\n",
        "    scraper.scrape_multiple_pages(urls, delay=delay)\n",
        "    return scraper.create_pdf(filename)"
      ],
      "metadata": {
        "id": "yRUgk5D1b8gL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demo_web_scraper():\n",
        "    \"\"\"Demo del web scraper\"\"\"\n",
        "    print(\"🧪 DEMO WEB SCRAPER\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # URLs de ejemplo (reemplaza con las tuyas)\n",
        "    #test_urls = [\n",
        "    #    \"https://ister.edu.ec/\",\n",
        "    #    \"https://ister.edu.ec/nosotros/\",\n",
        "    #    \"https://ister.edu.ec/informe-rendicion-de-cuentas/\",\n",
        "    #    \"https://ister.edu.ec/informe-rendicion-de-cuentas/#\",\n",
        "    #    \"https://ister.edu.ec/codigo-de-etica/\",\n",
        "    #    \"https://ister.edu.ec/normativa-general/\",\n",
        "    #    \"https://ister.edu.ec/normativa-institucional/\",\n",
        "    #    \"https://ister.edu.ec/programas-de-posgrado/\",\n",
        "    #    \"https://ister.edu.ec/oferta-tecnologias-superiores/\",\n",
        "    #    \"https://ister.edu.ec/oferta-tecnologias-superior/\",\n",
        "    #    \"https://ister.edu.ec/oferta-tecnicaturas-superiores/\",\n",
        "    #    \"https://ister.edu.ec/campus-norte/\",\n",
        "    #    \"https://ister.edu.ec/campus-sur/\",\n",
        "    #    \"https://ister.edu.ec/brochure-informativo/\",\n",
        "    #    \"https://ister.edu.ec/centros-de-apoyo/\",\n",
        "    #    \"https://ister.edu.ec/requisitos/\",\n",
        "    #    \"https://ister.edu.ec/financiamiento/\",\n",
        "    #    \"https://ister.edu.ec/homologacion-y-reingreso/\",\n",
        "    #    \"https://ister.edu.ec/vinculacion-universitario-ruminahui/\",\n",
        "    #    \"https://ister.edu.ec/programas-y-proyectos/\",\n",
        "    #    \"https://ister.edu.ec/practicas-pre-profesionales/\",\n",
        "    #    \"https://ister.edu.ec/educacion-continua-universitario-ruminahui/\",\n",
        "    #    \"https://ister.edu.ec/comunidad/\",\n",
        "    #    \"https://ister.edu.ec/bolsa-de-empleo/\",\n",
        "    #    \"https://ister.edu.ec/investigacion/\",\n",
        "    #    \"https://ister.edu.ec/investigacion/normativa-de-investigacion/\",\n",
        "    #    \"https://ister.edu.ec/investigacion/lineas-de-investigacion/\",\n",
        "    #    \"https://ister.edu.ec/proyectos-de-investigacion/\",\n",
        "    #    \"https://ister.edu.ec/investigacion/proyectos-de-investigacion/\",\n",
        "    #    \"https://ister.edu.ec/plan-operativo-anual/\",\n",
        "    #    \"https://ister.edu.ec/planificacion-institucional/\",\n",
        "    #    \"https://ister.edu.ec/bienestar-institucional-2/\",\n",
        "    #    \"https://ister.edu.ec/becas-y-ayudas-economicas/\",\n",
        "    #    \"https://ister.edu.ec/plan-de-mejoras-2024/\",\n",
        "    #    \"https://ister.edu.ec/plan-de-autoevaluacion/\",\n",
        "    #    \"https://ister.edu.ec/reglamento-de-aseguramiento-interno-de-la-calidad-del-universitario-ruminahui/\",\n",
        "    #    \"https://ister.edu.ec/institucion-acreditada/\",\n",
        "    #    \"https://ister.edu.ec/resultado-de-la-autoevaluacion/\",\n",
        "    #    \"https://ister.edu.ec/universidades-para-intercambios-internacionales-para-estudiantes-y-docentes/\"\n",
        "    #]\n",
        "\n",
        "\n",
        "    test_urls = [\n",
        "         \"https://es.wikipedia.org/wiki/Inteligencia_artificial\",\n",
        "        \"https://es.wikipedia.org/wiki/Aprendizaje_automático\",\n",
        "        \"https://es.wikipedia.org/wiki/Red_neuronal_artificial\"\n",
        "    ]\n",
        "\n",
        "    print(\"🌐 URLs de prueba:\")\n",
        "    for i, url in enumerate(test_urls, 1):\n",
        "        print(f\"  {i}. {url}\")\n",
        "\n",
        "    # Crear scraper\n",
        "    scraper = WebScraperToPDF()\n",
        "\n",
        "    # Scrapear páginas\n",
        "    results = scraper.scrape_multiple_pages(test_urls, delay=1.0)\n",
        "\n",
        "    # Crear PDF\n",
        "    pdf_file = scraper.create_pdf(\"demo_content.pdf\")\n",
        "\n",
        "    return scraper, pdf_file\n",
        "\n",
        "print(\"🚀 Web Scraper listo!\")\n",
        "print(\"\\n💡 Para usar:\")\n",
        "print(\"scraper = WebScraperToPDF()\")\n",
        "print(\"urls = ['url1', 'url2', 'url3']\")\n",
        "print(\"scraper.scrape_multiple_pages(urls)\")\n",
        "print(\"pdf_file = scraper.create_pdf('mi_contenido.pdf')\")\n",
        "print(\"\\n🚀 O usa la función rápida:\")\n",
        "print(\"pdf_file = quick_scrape_to_pdf(['url1', 'url2'], 'content.pdf')\")\n",
        "\n",
        "# Ejecutar demo automáticamente\n",
        "print(\"🚀 Ejecutando demo...\")\n",
        "demo_scraper, demo_pdf = demo_web_scraper()\n",
        "\n",
        "# Verificar si el PDF se creó\n",
        "import os\n",
        "if demo_pdf and os.path.exists(demo_pdf):\n",
        "    print(f\"\\n🎉 ¡PDF creado exitosamente!\")\n",
        "    print(f\"📁 Ubicación: {demo_pdf}\")\n",
        "    print(f\"📊 Tamaño: {os.path.getsize(demo_pdf)} bytes\")\n",
        "\n",
        "    # Mostrar contenido del directorio actual\n",
        "    print(f\"\\n📂 Archivos en el directorio actual:\")\n",
        "    files = [f for f in os.listdir('.') if f.endswith('.pdf')]\n",
        "    for file in files:\n",
        "        print(f\"   📄 {file}\")\n",
        "else:\n",
        "    print(\"❌ No se pudo crear el PDF\")\n",
        "\n",
        "print(\"\\n💡 Para descargar el PDF en Colab:\")\n",
        "print(\"from google.colab import files\")\n",
        "print(\"files.download('demo_content.pdf')\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZIoTQoKcAlB",
        "outputId": "287496a0-874a-49d1-81e5-dbb43082c342"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Web Scraper listo!\n",
            "\n",
            "💡 Para usar:\n",
            "scraper = WebScraperToPDF()\n",
            "urls = ['url1', 'url2', 'url3']\n",
            "scraper.scrape_multiple_pages(urls)\n",
            "pdf_file = scraper.create_pdf('mi_contenido.pdf')\n",
            "\n",
            "🚀 O usa la función rápida:\n",
            "pdf_file = quick_scrape_to_pdf(['url1', 'url2'], 'content.pdf')\n",
            "🚀 Ejecutando demo...\n",
            "🧪 DEMO WEB SCRAPER\n",
            "==================================================\n",
            "🌐 URLs de prueba:\n",
            "  1. https://es.wikipedia.org/wiki/Inteligencia_artificial\n",
            "  2. https://es.wikipedia.org/wiki/Aprendizaje_automático\n",
            "  3. https://es.wikipedia.org/wiki/Red_neuronal_artificial\n",
            "🚀 Iniciando scraping de 3 páginas...\n",
            "\n",
            "📄 Página 1/3\n",
            "🌐 Scrapeando: https://es.wikipedia.org/wiki/Inteligencia_artificial\n",
            "✅ Extraído: 5003 caracteres\n",
            "⏳ Esperando 1.0s...\n",
            "\n",
            "📄 Página 2/3\n",
            "🌐 Scrapeando: https://es.wikipedia.org/wiki/Aprendizaje_automático\n",
            "✅ Extraído: 5003 caracteres\n",
            "⏳ Esperando 1.0s...\n",
            "\n",
            "📄 Página 3/3\n",
            "🌐 Scrapeando: https://es.wikipedia.org/wiki/Red_neuronal_artificial\n",
            "✅ Extraído: 5003 caracteres\n",
            "\n",
            "✅ Scraping completado!\n",
            "   📊 Páginas exitosas: 3/3\n",
            "   📝 Total de caracteres: 15,009\n",
            "📄 Creando PDF: demo_content.pdf\n",
            "✅ PDF creado exitosamente: demo_content.pdf\n",
            "\n",
            "🎉 ¡PDF creado exitosamente!\n",
            "📁 Ubicación: demo_content.pdf\n",
            "📊 Tamaño: 12217 bytes\n",
            "\n",
            "📂 Archivos en el directorio actual:\n",
            "   📄 demo_content.pdf\n",
            "\n",
            "💡 Para descargar el PDF en Colab:\n",
            "from google.colab import files\n",
            "files.download('demo_content.pdf')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PASO 2: SISTEMA RAG CON PDF GENERADO\n",
        "# ============================================================================\n",
        "\n",
        "# Instalaciones adicionales para PDF\n",
        "!pip install -q PyPDF2 pdfplumber\n",
        "!pip install -q chromadb  # Vector database fácil para PoC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANvm6GgjdBOB",
        "outputId": "5cdf1437-aaa4-48bc-ad38-39730d48c029"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import pdfplumber\n",
        "import numpy as np\n",
        "import faiss\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from typing import List, Dict\n",
        "import re\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "wtxxN2Oagf7z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PDFRAGSystem:\n",
        "    def __init__(self, use_vector_db=True):\n",
        "        print(\"🚀 Inicializando Sistema RAG para PDF...\")\n",
        "\n",
        "        # Modelo de embeddings\n",
        "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        print(\"✅ Modelo de embeddings cargado\")\n",
        "\n",
        "        # Configurar almacenamiento\n",
        "        self.use_vector_db = use_vector_db\n",
        "        self.chunks = []\n",
        "\n",
        "        if use_vector_db:\n",
        "            print(\"📊 Configurando ChromaDB...\")\n",
        "            self.client = chromadb.Client()\n",
        "            try:\n",
        "                self.client.delete_collection(\"pdf_docs\")\n",
        "            except:\n",
        "                pass  # Collection no existe\n",
        "            self.collection = self.client.create_collection(\n",
        "                name=\"pdf_docs\",\n",
        "                metadata={\"hnsw:space\": \"cosine\"}\n",
        "            )\n",
        "        else:\n",
        "            print(\"📁 Configurando FAISS...\")\n",
        "            self.index = None\n",
        "            self.metadata = []\n",
        "\n",
        "        print(\"✅ Sistema RAG listo!\")\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path: str) -> List[Dict]:\n",
        "        \"\"\"Extraer texto del PDF de manera inteligente\"\"\"\n",
        "        print(f\"📄 Extrayendo texto de: {pdf_path}\")\n",
        "\n",
        "        if not os.path.exists(pdf_path):\n",
        "            print(f\"❌ Archivo no encontrado: {pdf_path}\")\n",
        "            return []\n",
        "\n",
        "        chunks = []\n",
        "\n",
        "        try:\n",
        "            # Método 1: Usar pdfplumber (mejor para texto estructurado)\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                for page_num, page in enumerate(pdf.pages):\n",
        "                    text = page.extract_text()\n",
        "\n",
        "                    if text and len(text.strip()) > 50:\n",
        "                        # Limpiar texto\n",
        "                        text = self._clean_extracted_text(text)\n",
        "\n",
        "                        # Dividir en chunks inteligentes\n",
        "                        page_chunks = self._smart_text_splitting(text, page_num + 1)\n",
        "                        chunks.extend(page_chunks)\n",
        "\n",
        "            print(f\"✅ Texto extraído: {len(chunks)} chunks de {len(pdf.pages)} páginas\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error con pdfplumber, intentando PyPDF2: {e}\")\n",
        "\n",
        "            # Método 2: Backup con PyPDF2\n",
        "            try:\n",
        "                with open(pdf_path, 'rb') as file:\n",
        "                    pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "                    for page_num, page in enumerate(pdf_reader.pages):\n",
        "                        text = page.extract_text()\n",
        "\n",
        "                        if text and len(text.strip()) > 50:\n",
        "                            text = self._clean_extracted_text(text)\n",
        "                            page_chunks = self._smart_text_splitting(text, page_num + 1)\n",
        "                            chunks.extend(page_chunks)\n",
        "\n",
        "                print(f\"✅ Texto extraído con PyPDF2: {len(chunks)} chunks\")\n",
        "\n",
        "            except Exception as e2:\n",
        "                print(f\"❌ Error extrayendo PDF: {e2}\")\n",
        "                return []\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _clean_extracted_text(self, text: str) -> str:\n",
        "        \"\"\"Limpiar texto extraído del PDF\"\"\"\n",
        "        # Remover saltos de línea excesivos\n",
        "        text = re.sub(r'\\n+', '\\n', text)\n",
        "        # Remover espacios múltiples\n",
        "        text = re.sub(r' +', ' ', text)\n",
        "        # Remover caracteres extraños comunes en PDFs\n",
        "        text = re.sub(r'[^\\w\\s\\.\\,\\;\\:\\!\\?\\-\\(\\)áéíóúñÁÉÍÓÚÑ\\n]', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def _smart_text_splitting(self, text: str, page_num: int, chunk_size: int = 500) -> List[Dict]:\n",
        "        \"\"\"Dividir texto de manera inteligente\"\"\"\n",
        "        chunks = []\n",
        "\n",
        "        # Dividir por párrafos primero\n",
        "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "\n",
        "        current_chunk = \"\"\n",
        "        chunk_counter = 1\n",
        "\n",
        "        for paragraph in paragraphs:\n",
        "            # Si el párrafo cabe en el chunk actual\n",
        "            if len(current_chunk) + len(paragraph) < chunk_size:\n",
        "                current_chunk += paragraph + \"\\n\\n\"\n",
        "            else:\n",
        "                # Guardar chunk actual si no está vacío\n",
        "                if current_chunk.strip():\n",
        "                    chunks.append({\n",
        "                        'content': current_chunk.strip(),\n",
        "                        'page': page_num,\n",
        "                        'chunk': chunk_counter,\n",
        "                        'source': 'pdf',\n",
        "                        'type': 'text'\n",
        "                    })\n",
        "                    chunk_counter += 1\n",
        "\n",
        "                # Iniciar nuevo chunk\n",
        "                current_chunk = paragraph + \"\\n\\n\"\n",
        "\n",
        "        # Guardar último chunk\n",
        "        if current_chunk.strip():\n",
        "            chunks.append({\n",
        "                'content': current_chunk.strip(),\n",
        "                'page': page_num,\n",
        "                'chunk': chunk_counter,\n",
        "                'source': 'pdf',\n",
        "                'type': 'text'\n",
        "            })\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def index_pdf(self, pdf_path: str):\n",
        "        \"\"\"Indexar contenido del PDF\"\"\"\n",
        "        print(f\"🔍 Indexando PDF: {pdf_path}\")\n",
        "\n",
        "        # Extraer chunks del PDF\n",
        "        chunks = self.extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        if not chunks:\n",
        "            print(\"❌ No se pudo extraer contenido del PDF\")\n",
        "            return False\n",
        "\n",
        "        self.chunks = chunks\n",
        "\n",
        "        # Generar embeddings\n",
        "        print(\"🧠 Generando embeddings...\")\n",
        "        texts = [chunk['content'] for chunk in chunks]\n",
        "        embeddings = self.embedder.encode(texts, show_progress_bar=True)\n",
        "\n",
        "        if self.use_vector_db:\n",
        "            # ChromaDB\n",
        "            ids = [f\"chunk_{i}\" for i in range(len(chunks))]\n",
        "            metadatas = [{k: v for k, v in chunk.items() if k != 'content'} for chunk in chunks]\n",
        "\n",
        "            self.collection.add(\n",
        "                embeddings=embeddings.tolist(),\n",
        "                documents=texts,\n",
        "                metadatas=metadatas,\n",
        "                ids=ids\n",
        "            )\n",
        "            print(f\"✅ {len(chunks)} chunks indexados en ChromaDB\")\n",
        "        else:\n",
        "            # FAISS\n",
        "            dimension = embeddings.shape[1]\n",
        "            self.index = faiss.IndexFlatL2(dimension)\n",
        "            self.index.add(embeddings.astype('float32'))\n",
        "            self.metadata = chunks\n",
        "            print(f\"✅ {len(chunks)} chunks indexados en FAISS\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def search(self, query: str, k: int = 3) -> List[Dict]:\n",
        "        \"\"\"Buscar chunks relevantes\"\"\"\n",
        "        if not self.chunks:\n",
        "            print(\"⚠️ No hay contenido indexado\")\n",
        "            return []\n",
        "\n",
        "        # Generar embedding de la consulta\n",
        "        query_embedding = self.embedder.encode([query])\n",
        "\n",
        "        if self.use_vector_db:\n",
        "            # ChromaDB\n",
        "            results = self.collection.query(\n",
        "                query_embeddings=query_embedding.tolist(),\n",
        "                n_results=min(k, len(self.chunks))\n",
        "            )\n",
        "\n",
        "            search_results = []\n",
        "            for i in range(len(results['documents'][0])):\n",
        "                search_results.append({\n",
        "                    'content': results['documents'][0][i],\n",
        "                    'metadata': results['metadatas'][0][i],\n",
        "                    'score': results['distances'][0][i]\n",
        "                })\n",
        "        else:\n",
        "            # FAISS\n",
        "            scores, indices = self.index.search(query_embedding.astype('float32'), min(k, len(self.chunks)))\n",
        "\n",
        "            search_results = []\n",
        "            for i in range(len(indices[0])):\n",
        "                if indices[0][i] < len(self.chunks):\n",
        "                    search_results.append({\n",
        "                        'content': self.chunks[indices[0][i]]['content'],\n",
        "                        'metadata': self.chunks[indices[0][i]],\n",
        "                        'score': float(scores[0][i])\n",
        "                    })\n",
        "\n",
        "        return search_results\n",
        "\n",
        "    def generate_answer(self, query: str, search_results: List[Dict]) -> str:\n",
        "        \"\"\"Generar respuesta basada en el contexto\"\"\"\n",
        "        if not search_results:\n",
        "            return \"No se encontró información relevante en el PDF para responder la pregunta.\"\n",
        "\n",
        "        # Construir contexto\n",
        "        context_parts = []\n",
        "        sources = []\n",
        "\n",
        "        for result in search_results[:3]:  # Top 3 resultados\n",
        "            content = result['content']\n",
        "            metadata = result['metadata']\n",
        "\n",
        "            context_parts.append(content)\n",
        "            sources.append(f\"(Página {metadata['page']}, Chunk {metadata['chunk']})\")\n",
        "\n",
        "        context = \" \".join(context_parts)\n",
        "\n",
        "        # Análisis simple de la consulta para generar respuesta\n",
        "        query_lower = query.lower()\n",
        "        context_lower = context.lower()\n",
        "\n",
        "        # Buscar respuestas específicas en el contexto\n",
        "        sentences = [s.strip() + '.' for s in context.split('.') if s.strip()]\n",
        "        relevant_sentences = []\n",
        "\n",
        "        # Palabras clave de la consulta\n",
        "        query_words = [word for word in query_lower.split() if len(word) > 3]\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_lower = sentence.lower()\n",
        "            # Contar coincidencias de palabras clave\n",
        "            matches = sum(1 for word in query_words if word in sentence_lower)\n",
        "            if matches > 0:\n",
        "                relevant_sentences.append((sentence, matches))\n",
        "\n",
        "        # Ordenar por relevancia\n",
        "        relevant_sentences.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        if relevant_sentences:\n",
        "            # Tomar las 2-3 oraciones más relevantes\n",
        "            answer_parts = [sent[0] for sent in relevant_sentences[:3]]\n",
        "            answer = \" \".join(answer_parts)\n",
        "        else:\n",
        "            # Respuesta por defecto con las primeras oraciones del contexto\n",
        "            answer = \". \".join(sentences[:2]) + \".\"\n",
        "\n",
        "        # Agregar fuentes\n",
        "        answer += f\" Fuentes: {', '.join(sources[:2])}\"\n",
        "\n",
        "        return answer\n",
        "\n",
        "    def query(self, question: str, k: int = 3, verbose: bool = True) -> Dict:\n",
        "        \"\"\"Consulta RAG completa\"\"\"\n",
        "        if verbose:\n",
        "            print(f\"\\n❓ Pregunta: {question}\")\n",
        "\n",
        "        # Buscar contenido relevante\n",
        "        search_results = self.search(question, k)\n",
        "\n",
        "        if verbose and search_results:\n",
        "            print(f\"📚 Encontrados {len(search_results)} chunks relevantes:\")\n",
        "            for i, result in enumerate(search_results[:2]):\n",
        "                meta = result['metadata']\n",
        "                print(f\"  {i+1}. Página {meta['page']}, Chunk {meta['chunk']} (Score: {result['score']:.3f})\")\n",
        "\n",
        "        # Generar respuesta\n",
        "        answer = self.generate_answer(question, search_results)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"💡 Respuesta: {answer}\")\n",
        "\n",
        "        return {\n",
        "            'question': question,\n",
        "            'answer': answer,\n",
        "            'sources': search_results,\n",
        "            'context': \" \".join([r['content'] for r in search_results[:2]])\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# FUNCIÓN RÁPIDA PARA USAR CON EL PDF GENERADO\n",
        "# ============================================================================\n",
        "\n",
        "def quick_pdf_rag(pdf_path: str = \"demo_content.pdf\"):\n",
        "    \"\"\"Función rápida para crear RAG con el PDF\"\"\"\n",
        "    print(\"🚀 Configurando RAG rápido para PDF...\")\n",
        "\n",
        "    # Verificar que el PDF existe\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(f\"❌ PDF no encontrado: {pdf_path}\")\n",
        "        print(\"📂 PDFs disponibles:\")\n",
        "        pdfs = [f for f in os.listdir('.') if f.endswith('.pdf')]\n",
        "        for pdf in pdfs:\n",
        "            print(f\"   📄 {pdf}\")\n",
        "        return None\n",
        "\n",
        "    # Crear sistema RAG\n",
        "    rag = PDFRAGSystem(use_vector_db=True)\n",
        "\n",
        "    # Indexar PDF\n",
        "    success = rag.index_pdf(pdf_path)\n",
        "\n",
        "    if not success:\n",
        "        print(\"❌ Error indexando PDF\")\n",
        "        return None\n",
        "\n",
        "    print(\"✅ RAG listo para consultas!\")\n",
        "    return rag"
      ],
      "metadata": {
        "id": "EUZV_qKwgifq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DEMO AUTOMÁTICO\n",
        "# ============================================================================\n",
        "\n",
        "print(\"🧪 INICIANDO DEMO RAG CON PDF\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Crear RAG con el PDF generado\n",
        "pdf_rag = quick_pdf_rag(\"demo_content.pdf\")\n",
        "\n",
        "if pdf_rag:\n",
        "    # Preguntas de prueba\n",
        "    test_questions = [\n",
        "        \"¿Qué es la inteligencia artificial?\",\n",
        "        \"¿Cómo funciona el aprendizaje automático?\",\n",
        "        \"¿Qué son las redes neuronales?\",\n",
        "        \"¿Cuáles son las aplicaciones de la IA?\"\n",
        "    ]\n",
        "\n",
        "    print(f\"\\n🧪 PROBANDO {len(test_questions)} PREGUNTAS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for i, question in enumerate(test_questions, 1):\n",
        "        print(f\"\\n--- Pregunta {i} ---\")\n",
        "        result = pdf_rag.query(question)\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    print(\"\\n🎉 Demo completado!\")\n",
        "    print(\"\\n💡 Para usar:\")\n",
        "    print(\"result = pdf_rag.query('tu pregunta aquí')\")\n",
        "    print(\"print(result['answer'])\")\n",
        "else:\n",
        "    print(\"❌ No se pudo configurar el RAG. Verifica que el PDF existe.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "53ededcb128a42cd9c8dd02ed67385a7",
            "a206fcef7de74de28207b0ed8ff70685",
            "6fc0e415bb04436aa49af026e859d18d",
            "3028cabdd9fd4258a956620b79613c7e",
            "2fab19445ef94af799ef6ad0eb67a805",
            "c94356b7de1a4cdaaebe4133751e879a",
            "33a3cf79a0a544be85798777ce8edc5e",
            "4a746c9e41494113b3d53d86d950d8c9",
            "517ed125753142e8a8e1b316712d8393",
            "c21c192e55914adbb8e7216a9c99b836",
            "ceca7e8853aa446ca79441e4f014239e"
          ]
        },
        "id": "3XuP8SljgwgK",
        "outputId": "a9acad30-4da1-42b2-b34d-0b5b28565495"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 INICIANDO DEMO RAG CON PDF\n",
            "==================================================\n",
            "🚀 Configurando RAG rápido para PDF...\n",
            "🚀 Inicializando Sistema RAG para PDF...\n",
            "✅ Modelo de embeddings cargado\n",
            "📊 Configurando ChromaDB...\n",
            "✅ Sistema RAG listo!\n",
            "🔍 Indexando PDF: demo_content.pdf\n",
            "📄 Extrayendo texto de: demo_content.pdf\n",
            "✅ Texto extraído: 7 chunks de 7 páginas\n",
            "🧠 Generando embeddings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53ededcb128a42cd9c8dd02ed67385a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 7 chunks indexados en ChromaDB\n",
            "✅ RAG listo para consultas!\n",
            "\n",
            "🧪 PROBANDO 4 PREGUNTAS\n",
            "==================================================\n",
            "\n",
            "--- Pregunta 1 ---\n",
            "\n",
            "❓ Pregunta: ¿Qué es la inteligencia artificial?\n",
            "📚 Encontrados 3 chunks relevantes:\n",
            "  1. Página 2, Chunk 1 (Score: 0.346)\n",
            "  2. Página 1, Chunk 1 (Score: 0.437)\n",
            "💡 Respuesta: Inteligencia artificial - Wikipedia, la enciclopedia libre\n",
            "Fuente: https:  es. org wiki Inteligencia_artificial\n",
            "De Wikipedia, la enciclopedia libre Imagen generada por la inteligencia\n",
            "artificial Dalle 3. Vídeo explicativo de 6:47 min, en idioma euskera (con\n",
            "subtítulos en castellano) sobre la inteligencia artificial, incluyendo\n",
            "secciones sobre los dilemas éticos. Fuentes: (Página 2, Chunk 1), (Página 1, Chunk 1)\n",
            "----------------------------------------\n",
            "\n",
            "--- Pregunta 2 ---\n",
            "\n",
            "❓ Pregunta: ¿Cómo funciona el aprendizaje automático?\n",
            "📚 Encontrados 3 chunks relevantes:\n",
            "  1. Página 4, Chunk 1 (Score: 0.306)\n",
            "  2. Página 5, Chunk 1 (Score: 0.357)\n",
            "💡 Respuesta: Aprendizaje automático - Wikipedia, la enciclopedia libre\n",
            "Fuente: https:  es. org wiki Aprendizaje_automático\n",
            "De Wikipedia, la enciclopedia libre El aprendizaje automático (AA); también\n",
            "llamado automatizado, computacional de máquinas, o maquinal1 (del inglés\n",
            "machine learning, ML), es el subcampo de las ciencias de la computación y una\n",
            "rama de la inteligencia artificial, cuyo objetivo es desarrollar técnicas que\n",
            "permitan que las computadoras aprendan. 2\n",
            "En el aprendizaje de máquinas un computador observa datos, construye un modelo\n",
            "basado en esos datos y utiliza ese modelo a la vez como una hipótesis acerca\n",
            "del mundo y una pieza de software que puede resolver problemas. Fuentes: (Página 4, Chunk 1), (Página 5, Chunk 1)\n",
            "----------------------------------------\n",
            "\n",
            "--- Pregunta 3 ---\n",
            "\n",
            "❓ Pregunta: ¿Qué son las redes neuronales?\n",
            "📚 Encontrados 3 chunks relevantes:\n",
            "  1. Página 7, Chunk 1 (Score: 0.498)\n",
            "  2. Página 6, Chunk 1 (Score: 0.507)\n",
            "💡 Respuesta: Las redes neuronales actuales suelen contener desde unos miles a\n",
            "unos pocos millones de unidades neuronales. Nuevas investigaciones sobre el\n",
            "cerebro a menudo estimulan la creación de nuevos patrones en las redes\n",
            "neuronalescita requerida. Las redes neuronales se han utilizado para resolver una\n",
            "amplia variedad de tareas, como la visión por computador y el reconocimiento de\n",
            "voz, que son difíciles de resolver usando la ordinaria programación basada en\n",
            "reglas. Fuentes: (Página 7, Chunk 1), (Página 6, Chunk 1)\n",
            "----------------------------------------\n",
            "\n",
            "--- Pregunta 4 ---\n",
            "\n",
            "❓ Pregunta: ¿Cuáles son las aplicaciones de la IA?\n",
            "📚 Encontrados 3 chunks relevantes:\n",
            "  1. Página 2, Chunk 1 (Score: 0.527)\n",
            "  2. Página 4, Chunk 1 (Score: 0.566)\n",
            "💡 Respuesta: El\n",
            "aprendizaje automático tiene una amplia gama de aplicaciones, incluyendo\n",
            "motores de búsqueda, diagnósticos médicos, detección de fraude en el uso de\n",
            "tarjetas de crédito, análisis de mercado para los diferentes sectores de\n",
            "actividad, clasificación de secuencias de ADN, reconocimiento del habla y del\n",
            "lenguaje escrito, juegos y robótica. Fuentes: (Página 2, Chunk 1), (Página 4, Chunk 1)\n",
            "----------------------------------------\n",
            "\n",
            "🎉 Demo completado!\n",
            "\n",
            "💡 Para usar:\n",
            "result = pdf_rag.query('tu pregunta aquí')\n",
            "print(result['answer'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dDk7fc9yg6OK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}