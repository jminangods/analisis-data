{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmQ/qk+NXxMJUjHzDG6sAa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "53ededcb128a42cd9c8dd02ed67385a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a206fcef7de74de28207b0ed8ff70685",
              "IPY_MODEL_6fc0e415bb04436aa49af026e859d18d",
              "IPY_MODEL_3028cabdd9fd4258a956620b79613c7e"
            ],
            "layout": "IPY_MODEL_2fab19445ef94af799ef6ad0eb67a805"
          }
        },
        "a206fcef7de74de28207b0ed8ff70685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c94356b7de1a4cdaaebe4133751e879a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_33a3cf79a0a544be85798777ce8edc5e",
            "value": "Batches:â€‡100%"
          }
        },
        "6fc0e415bb04436aa49af026e859d18d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a746c9e41494113b3d53d86d950d8c9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_517ed125753142e8a8e1b316712d8393",
            "value": 1
          }
        },
        "3028cabdd9fd4258a956620b79613c7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c21c192e55914adbb8e7216a9c99b836",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ceca7e8853aa446ca79441e4f014239e",
            "value": "â€‡1/1â€‡[00:00&lt;00:00,â€‡â€‡1.01it/s]"
          }
        },
        "2fab19445ef94af799ef6ad0eb67a805": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c94356b7de1a4cdaaebe4133751e879a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33a3cf79a0a544be85798777ce8edc5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a746c9e41494113b3d53d86d950d8c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "517ed125753142e8a8e1b316712d8393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c21c192e55914adbb8e7216a9c99b836": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceca7e8853aa446ca79441e4f014239e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jminangods/analisis-data/blob/main/RAG_Completo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSJgY-yrVSYX",
        "outputId": "b16cd106-8c65-48e5-af39-fe70bad59172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.7/72.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m251.7/251.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PASO 1: WEB SCRAPER ROBUSTO PARA MÃšLTIPLES PÃGINAS\n",
        "# ============================================================================\n",
        "\n",
        "# Instalaciones para Colab\n",
        "!pip install -q requests beautifulsoup4 fpdf2 sentence-transformers faiss-cpu torch\n",
        "!pip install -q lxml html5lib  # Parsers adicionales para BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from fpdf import FPDF\n",
        "import time\n",
        "import re\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from typing import List, Dict, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "zmX6O5FocUcd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WebScraperToPDF:\n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        })\n",
        "        self.scraped_content = []\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Limpiar texto extraÃ­do\"\"\"\n",
        "        # Remover espacios extra y saltos de lÃ­nea\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        # Remover caracteres especiales problemÃ¡ticos\n",
        "        text = re.sub(r'[^\\w\\s\\.\\,\\;\\:\\!\\?\\-\\(\\)Ã¡Ã©Ã­Ã³ÃºÃ±ÃÃ‰ÃÃ“ÃšÃ‘]', '', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def scrape_single_page(self, url: str, max_length: int = 5000) -> Dict:\n",
        "        \"\"\"Scrapear una pÃ¡gina individual\"\"\"\n",
        "        print(f\"ðŸŒ Scrapeando: {url}\")\n",
        "\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            response.encoding = 'utf-8'\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Remover elementos no deseados\n",
        "            for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):\n",
        "                element.decompose()\n",
        "\n",
        "            # Extraer tÃ­tulo\n",
        "            title = soup.find('title')\n",
        "            title = title.get_text().strip() if title else f\"PÃ¡gina de {urlparse(url).netloc}\"\n",
        "\n",
        "            # Extraer contenido principal\n",
        "            # Buscar contenedores comunes de contenido\n",
        "            content_selectors = [\n",
        "                'main', 'article', '.content', '.post', '.entry-content',\n",
        "                '[role=\"main\"]', '.main-content', '#content', '.page-content'\n",
        "            ]\n",
        "\n",
        "            content_element = None\n",
        "            for selector in content_selectors:\n",
        "                content_element = soup.select_one(selector)\n",
        "                if content_element:\n",
        "                    break\n",
        "\n",
        "            # Si no encuentra contenedor especÃ­fico, usar body\n",
        "            if not content_element:\n",
        "                content_element = soup.find('body')\n",
        "\n",
        "            if content_element:\n",
        "                # Extraer texto de pÃ¡rrafos, headers, listas\n",
        "                text_elements = content_element.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'div'])\n",
        "\n",
        "                content_parts = []\n",
        "                for element in text_elements:\n",
        "                    text = element.get_text().strip()\n",
        "                    if len(text) > 20:  # Solo textos significativos\n",
        "                        content_parts.append(text)\n",
        "\n",
        "                content = '\\n\\n'.join(content_parts)\n",
        "            else:\n",
        "                content = soup.get_text()\n",
        "\n",
        "            # Limpiar y truncar contenido\n",
        "            content = self.clean_text(content)\n",
        "            if len(content) > max_length:\n",
        "                content = content[:max_length] + \"...\"\n",
        "\n",
        "            print(f\"âœ… ExtraÃ­do: {len(content)} caracteres\")\n",
        "\n",
        "            return {\n",
        "                'url': url,\n",
        "                'title': title,\n",
        "                'content': content,\n",
        "                'success': True,\n",
        "                'length': len(content)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error en {url}: {str(e)}\")\n",
        "            return {\n",
        "                'url': url,\n",
        "                'title': f\"Error: {urlparse(url).netloc}\",\n",
        "                'content': f\"No se pudo extraer contenido de {url}. Error: {str(e)}\",\n",
        "                'success': False,\n",
        "                'length': 0\n",
        "            }\n",
        "\n",
        "    def scrape_multiple_pages(self, urls: List[str], delay: float = 1.0) -> List[Dict]:\n",
        "        \"\"\"Scrapear mÃºltiples pÃ¡ginas con delay\"\"\"\n",
        "        print(f\"ðŸš€ Iniciando scraping de {len(urls)} pÃ¡ginas...\")\n",
        "\n",
        "        results = []\n",
        "        for i, url in enumerate(urls, 1):\n",
        "            print(f\"\\nðŸ“„ PÃ¡gina {i}/{len(urls)}\")\n",
        "\n",
        "            result = self.scrape_single_page(url)\n",
        "            results.append(result)\n",
        "            self.scraped_content.append(result)\n",
        "\n",
        "            # Delay entre requests para ser respetuoso\n",
        "            if i < len(urls):\n",
        "                print(f\"â³ Esperando {delay}s...\")\n",
        "                time.sleep(delay)\n",
        "\n",
        "        successful = sum(1 for r in results if r['success'])\n",
        "        total_chars = sum(r['length'] for r in results)\n",
        "\n",
        "        print(f\"\\nâœ… Scraping completado!\")\n",
        "        print(f\"   ðŸ“Š PÃ¡ginas exitosas: {successful}/{len(urls)}\")\n",
        "        print(f\"   ðŸ“ Total de caracteres: {total_chars:,}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def create_pdf(self, output_filename: str = \"scraped_content.pdf\") -> str:\n",
        "        \"\"\"Crear PDF con todo el contenido scrapeado\"\"\"\n",
        "        print(f\"ðŸ“„ Creando PDF: {output_filename}\")\n",
        "\n",
        "        if not self.scraped_content:\n",
        "            print(\"âŒ No hay contenido para crear PDF\")\n",
        "            return None\n",
        "\n",
        "        # Configurar PDF\n",
        "        pdf = FPDF()\n",
        "        pdf.set_auto_page_break(auto=True, margin=15)\n",
        "\n",
        "        # PÃ¡gina de tÃ­tulo\n",
        "        pdf.add_page()\n",
        "        pdf.set_font('Arial', 'B', 16)\n",
        "        pdf.cell(0, 10, 'Contenido Web Consolidado', 0, 1, 'C')\n",
        "        pdf.set_font('Arial', '', 10)\n",
        "        pdf.cell(0, 10, f'Generado automÃ¡ticamente - {len(self.scraped_content)} pÃ¡ginas', 0, 1, 'C')\n",
        "        pdf.ln(10)\n",
        "\n",
        "        # Ãndice\n",
        "        pdf.set_font('Arial', 'B', 14)\n",
        "        pdf.cell(0, 10, 'Ãndice', 0, 1, 'L')\n",
        "        pdf.set_font('Arial', '', 10)\n",
        "\n",
        "        for i, page_data in enumerate(self.scraped_content, 1):\n",
        "            title = page_data['title'][:60] + \"...\" if len(page_data['title']) > 60 else page_data['title']\n",
        "            try:\n",
        "                pdf.cell(0, 6, f\"{i}. {title}\", 0, 1, 'L')\n",
        "            except:\n",
        "                pdf.cell(0, 6, f\"{i}. [TÃ­tulo con caracteres especiales]\", 0, 1, 'L')\n",
        "\n",
        "        # Contenido de cada pÃ¡gina\n",
        "        for i, page_data in enumerate(self.scraped_content, 1):\n",
        "            pdf.add_page()\n",
        "\n",
        "            # TÃ­tulo de la secciÃ³n\n",
        "            pdf.set_font('Arial', 'B', 14)\n",
        "            title = page_data['title']\n",
        "            try:\n",
        "                pdf.cell(0, 10, f\"{i}. {title}\", 0, 1, 'L')\n",
        "            except:\n",
        "                pdf.cell(0, 10, f\"{i}. [TÃ­tulo con caracteres especiales]\", 0, 1, 'L')\n",
        "\n",
        "            # URL\n",
        "            pdf.set_font('Arial', 'I', 8)\n",
        "            pdf.cell(0, 5, f\"Fuente: {page_data['url']}\", 0, 1, 'L')\n",
        "            pdf.ln(5)\n",
        "\n",
        "            # Contenido\n",
        "            pdf.set_font('Arial', '', 10)\n",
        "            content = page_data['content']\n",
        "\n",
        "            # Dividir contenido en lÃ­neas manejables\n",
        "            try:\n",
        "                # Intentar escribir contenido normal\n",
        "                lines = content.split('\\n')\n",
        "                for line in lines:\n",
        "                    if line.strip():\n",
        "                        # Manejar lÃ­neas largas\n",
        "                        words = line.split(' ')\n",
        "                        current_line = \"\"\n",
        "                        for word in words:\n",
        "                            if len(current_line + word) < 80:\n",
        "                                current_line += word + \" \"\n",
        "                            else:\n",
        "                                if current_line:\n",
        "                                    try:\n",
        "                                        pdf.cell(0, 5, current_line.strip(), 0, 1, 'L')\n",
        "                                    except:\n",
        "                                        pdf.cell(0, 5, \"[LÃ­nea con caracteres especiales]\", 0, 1, 'L')\n",
        "                                current_line = word + \" \"\n",
        "                        if current_line:\n",
        "                            try:\n",
        "                                pdf.cell(0, 5, current_line.strip(), 0, 1, 'L')\n",
        "                            except:\n",
        "                                pdf.cell(0, 5, \"[LÃ­nea con caracteres especiales]\", 0, 1, 'L')\n",
        "                    pdf.ln(2)\n",
        "            except Exception as e:\n",
        "                pdf.cell(0, 5, f\"[Error mostrando contenido: {str(e)}]\", 0, 1, 'L')\n",
        "\n",
        "            pdf.ln(10)\n",
        "\n",
        "        # Guardar PDF\n",
        "        try:\n",
        "            pdf.output(output_filename)\n",
        "            print(f\"âœ… PDF creado exitosamente: {output_filename}\")\n",
        "            return output_filename\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error creando PDF: {e}\")\n",
        "            return None\n"
      ],
      "metadata": {
        "id": "vJN59t0lbkNZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FUNCIÃ“N HELPER PARA URLS FÃCILES\n",
        "# ============================================================================\n",
        "\n",
        "def quick_scrape_to_pdf(urls: List[str], filename: str = \"content.pdf\", delay: float = 1.0) -> str:\n",
        "    \"\"\"FunciÃ³n rÃ¡pida para scrapear URLs y crear PDF\"\"\"\n",
        "    scraper = WebScraperToPDF()\n",
        "    scraper.scrape_multiple_pages(urls, delay=delay)\n",
        "    return scraper.create_pdf(filename)"
      ],
      "metadata": {
        "id": "yRUgk5D1b8gL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demo_web_scraper():\n",
        "    \"\"\"Demo del web scraper\"\"\"\n",
        "    print(\"ðŸ§ª DEMO WEB SCRAPER\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # URLs de ejemplo (reemplaza con las tuyas)\n",
        "    #test_urls = [\n",
        "    #    \"https://ister.edu.ec/\",\n",
        "    #    \"https://ister.edu.ec/nosotros/\",\n",
        "    #    \"https://ister.edu.ec/informe-rendicion-de-cuentas/\",\n",
        "    #    \"https://ister.edu.ec/informe-rendicion-de-cuentas/#\",\n",
        "    #    \"https://ister.edu.ec/codigo-de-etica/\",\n",
        "    #    \"https://ister.edu.ec/normativa-general/\",\n",
        "    #    \"https://ister.edu.ec/normativa-institucional/\",\n",
        "    #    \"https://ister.edu.ec/programas-de-posgrado/\",\n",
        "    #    \"https://ister.edu.ec/oferta-tecnologias-superiores/\",\n",
        "    #    \"https://ister.edu.ec/oferta-tecnologias-superior/\",\n",
        "    #    \"https://ister.edu.ec/oferta-tecnicaturas-superiores/\",\n",
        "    #    \"https://ister.edu.ec/campus-norte/\",\n",
        "    #    \"https://ister.edu.ec/campus-sur/\",\n",
        "    #    \"https://ister.edu.ec/brochure-informativo/\",\n",
        "    #    \"https://ister.edu.ec/centros-de-apoyo/\",\n",
        "    #    \"https://ister.edu.ec/requisitos/\",\n",
        "    #    \"https://ister.edu.ec/financiamiento/\",\n",
        "    #    \"https://ister.edu.ec/homologacion-y-reingreso/\",\n",
        "    #    \"https://ister.edu.ec/vinculacion-universitario-ruminahui/\",\n",
        "    #    \"https://ister.edu.ec/programas-y-proyectos/\",\n",
        "    #    \"https://ister.edu.ec/practicas-pre-profesionales/\",\n",
        "    #    \"https://ister.edu.ec/educacion-continua-universitario-ruminahui/\",\n",
        "    #    \"https://ister.edu.ec/comunidad/\",\n",
        "    #    \"https://ister.edu.ec/bolsa-de-empleo/\",\n",
        "    #    \"https://ister.edu.ec/investigacion/\",\n",
        "    #    \"https://ister.edu.ec/investigacion/normativa-de-investigacion/\",\n",
        "    #    \"https://ister.edu.ec/investigacion/lineas-de-investigacion/\",\n",
        "    #    \"https://ister.edu.ec/proyectos-de-investigacion/\",\n",
        "    #    \"https://ister.edu.ec/investigacion/proyectos-de-investigacion/\",\n",
        "    #    \"https://ister.edu.ec/plan-operativo-anual/\",\n",
        "    #    \"https://ister.edu.ec/planificacion-institucional/\",\n",
        "    #    \"https://ister.edu.ec/bienestar-institucional-2/\",\n",
        "    #    \"https://ister.edu.ec/becas-y-ayudas-economicas/\",\n",
        "    #    \"https://ister.edu.ec/plan-de-mejoras-2024/\",\n",
        "    #    \"https://ister.edu.ec/plan-de-autoevaluacion/\",\n",
        "    #    \"https://ister.edu.ec/reglamento-de-aseguramiento-interno-de-la-calidad-del-universitario-ruminahui/\",\n",
        "    #    \"https://ister.edu.ec/institucion-acreditada/\",\n",
        "    #    \"https://ister.edu.ec/resultado-de-la-autoevaluacion/\",\n",
        "    #    \"https://ister.edu.ec/universidades-para-intercambios-internacionales-para-estudiantes-y-docentes/\"\n",
        "    #]\n",
        "\n",
        "\n",
        "    test_urls = [\n",
        "         \"https://es.wikipedia.org/wiki/Inteligencia_artificial\",\n",
        "        \"https://es.wikipedia.org/wiki/Aprendizaje_automÃ¡tico\",\n",
        "        \"https://es.wikipedia.org/wiki/Red_neuronal_artificial\"\n",
        "    ]\n",
        "\n",
        "    print(\"ðŸŒ URLs de prueba:\")\n",
        "    for i, url in enumerate(test_urls, 1):\n",
        "        print(f\"  {i}. {url}\")\n",
        "\n",
        "    # Crear scraper\n",
        "    scraper = WebScraperToPDF()\n",
        "\n",
        "    # Scrapear pÃ¡ginas\n",
        "    results = scraper.scrape_multiple_pages(test_urls, delay=1.0)\n",
        "\n",
        "    # Crear PDF\n",
        "    pdf_file = scraper.create_pdf(\"demo_content.pdf\")\n",
        "\n",
        "    return scraper, pdf_file\n",
        "\n",
        "print(\"ðŸš€ Web Scraper listo!\")\n",
        "print(\"\\nðŸ’¡ Para usar:\")\n",
        "print(\"scraper = WebScraperToPDF()\")\n",
        "print(\"urls = ['url1', 'url2', 'url3']\")\n",
        "print(\"scraper.scrape_multiple_pages(urls)\")\n",
        "print(\"pdf_file = scraper.create_pdf('mi_contenido.pdf')\")\n",
        "print(\"\\nðŸš€ O usa la funciÃ³n rÃ¡pida:\")\n",
        "print(\"pdf_file = quick_scrape_to_pdf(['url1', 'url2'], 'content.pdf')\")\n",
        "\n",
        "# Ejecutar demo automÃ¡ticamente\n",
        "print(\"ðŸš€ Ejecutando demo...\")\n",
        "demo_scraper, demo_pdf = demo_web_scraper()\n",
        "\n",
        "# Verificar si el PDF se creÃ³\n",
        "import os\n",
        "if demo_pdf and os.path.exists(demo_pdf):\n",
        "    print(f\"\\nðŸŽ‰ Â¡PDF creado exitosamente!\")\n",
        "    print(f\"ðŸ“ UbicaciÃ³n: {demo_pdf}\")\n",
        "    print(f\"ðŸ“Š TamaÃ±o: {os.path.getsize(demo_pdf)} bytes\")\n",
        "\n",
        "    # Mostrar contenido del directorio actual\n",
        "    print(f\"\\nðŸ“‚ Archivos en el directorio actual:\")\n",
        "    files = [f for f in os.listdir('.') if f.endswith('.pdf')]\n",
        "    for file in files:\n",
        "        print(f\"   ðŸ“„ {file}\")\n",
        "else:\n",
        "    print(\"âŒ No se pudo crear el PDF\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Para descargar el PDF en Colab:\")\n",
        "print(\"from google.colab import files\")\n",
        "print(\"files.download('demo_content.pdf')\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZIoTQoKcAlB",
        "outputId": "287496a0-874a-49d1-81e5-dbb43082c342"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Web Scraper listo!\n",
            "\n",
            "ðŸ’¡ Para usar:\n",
            "scraper = WebScraperToPDF()\n",
            "urls = ['url1', 'url2', 'url3']\n",
            "scraper.scrape_multiple_pages(urls)\n",
            "pdf_file = scraper.create_pdf('mi_contenido.pdf')\n",
            "\n",
            "ðŸš€ O usa la funciÃ³n rÃ¡pida:\n",
            "pdf_file = quick_scrape_to_pdf(['url1', 'url2'], 'content.pdf')\n",
            "ðŸš€ Ejecutando demo...\n",
            "ðŸ§ª DEMO WEB SCRAPER\n",
            "==================================================\n",
            "ðŸŒ URLs de prueba:\n",
            "  1. https://es.wikipedia.org/wiki/Inteligencia_artificial\n",
            "  2. https://es.wikipedia.org/wiki/Aprendizaje_automÃ¡tico\n",
            "  3. https://es.wikipedia.org/wiki/Red_neuronal_artificial\n",
            "ðŸš€ Iniciando scraping de 3 pÃ¡ginas...\n",
            "\n",
            "ðŸ“„ PÃ¡gina 1/3\n",
            "ðŸŒ Scrapeando: https://es.wikipedia.org/wiki/Inteligencia_artificial\n",
            "âœ… ExtraÃ­do: 5003 caracteres\n",
            "â³ Esperando 1.0s...\n",
            "\n",
            "ðŸ“„ PÃ¡gina 2/3\n",
            "ðŸŒ Scrapeando: https://es.wikipedia.org/wiki/Aprendizaje_automÃ¡tico\n",
            "âœ… ExtraÃ­do: 5003 caracteres\n",
            "â³ Esperando 1.0s...\n",
            "\n",
            "ðŸ“„ PÃ¡gina 3/3\n",
            "ðŸŒ Scrapeando: https://es.wikipedia.org/wiki/Red_neuronal_artificial\n",
            "âœ… ExtraÃ­do: 5003 caracteres\n",
            "\n",
            "âœ… Scraping completado!\n",
            "   ðŸ“Š PÃ¡ginas exitosas: 3/3\n",
            "   ðŸ“ Total de caracteres: 15,009\n",
            "ðŸ“„ Creando PDF: demo_content.pdf\n",
            "âœ… PDF creado exitosamente: demo_content.pdf\n",
            "\n",
            "ðŸŽ‰ Â¡PDF creado exitosamente!\n",
            "ðŸ“ UbicaciÃ³n: demo_content.pdf\n",
            "ðŸ“Š TamaÃ±o: 12217 bytes\n",
            "\n",
            "ðŸ“‚ Archivos en el directorio actual:\n",
            "   ðŸ“„ demo_content.pdf\n",
            "\n",
            "ðŸ’¡ Para descargar el PDF en Colab:\n",
            "from google.colab import files\n",
            "files.download('demo_content.pdf')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PASO 2: SISTEMA RAG CON PDF GENERADO\n",
        "# ============================================================================\n",
        "\n",
        "# Instalaciones adicionales para PDF\n",
        "!pip install -q PyPDF2 pdfplumber\n",
        "!pip install -q chromadb  # Vector database fÃ¡cil para PoC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANvm6GgjdBOB",
        "outputId": "5cdf1437-aaa4-48bc-ad38-39730d48c029"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import pdfplumber\n",
        "import numpy as np\n",
        "import faiss\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from typing import List, Dict\n",
        "import re\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "wtxxN2Oagf7z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PDFRAGSystem:\n",
        "    def __init__(self, use_vector_db=True):\n",
        "        print(\"ðŸš€ Inicializando Sistema RAG para PDF...\")\n",
        "\n",
        "        # Modelo de embeddings\n",
        "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        print(\"âœ… Modelo de embeddings cargado\")\n",
        "\n",
        "        # Configurar almacenamiento\n",
        "        self.use_vector_db = use_vector_db\n",
        "        self.chunks = []\n",
        "\n",
        "        if use_vector_db:\n",
        "            print(\"ðŸ“Š Configurando ChromaDB...\")\n",
        "            self.client = chromadb.Client()\n",
        "            try:\n",
        "                self.client.delete_collection(\"pdf_docs\")\n",
        "            except:\n",
        "                pass  # Collection no existe\n",
        "            self.collection = self.client.create_collection(\n",
        "                name=\"pdf_docs\",\n",
        "                metadata={\"hnsw:space\": \"cosine\"}\n",
        "            )\n",
        "        else:\n",
        "            print(\"ðŸ“ Configurando FAISS...\")\n",
        "            self.index = None\n",
        "            self.metadata = []\n",
        "\n",
        "        print(\"âœ… Sistema RAG listo!\")\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path: str) -> List[Dict]:\n",
        "        \"\"\"Extraer texto del PDF de manera inteligente\"\"\"\n",
        "        print(f\"ðŸ“„ Extrayendo texto de: {pdf_path}\")\n",
        "\n",
        "        if not os.path.exists(pdf_path):\n",
        "            print(f\"âŒ Archivo no encontrado: {pdf_path}\")\n",
        "            return []\n",
        "\n",
        "        chunks = []\n",
        "\n",
        "        try:\n",
        "            # MÃ©todo 1: Usar pdfplumber (mejor para texto estructurado)\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                for page_num, page in enumerate(pdf.pages):\n",
        "                    text = page.extract_text()\n",
        "\n",
        "                    if text and len(text.strip()) > 50:\n",
        "                        # Limpiar texto\n",
        "                        text = self._clean_extracted_text(text)\n",
        "\n",
        "                        # Dividir en chunks inteligentes\n",
        "                        page_chunks = self._smart_text_splitting(text, page_num + 1)\n",
        "                        chunks.extend(page_chunks)\n",
        "\n",
        "            print(f\"âœ… Texto extraÃ­do: {len(chunks)} chunks de {len(pdf.pages)} pÃ¡ginas\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error con pdfplumber, intentando PyPDF2: {e}\")\n",
        "\n",
        "            # MÃ©todo 2: Backup con PyPDF2\n",
        "            try:\n",
        "                with open(pdf_path, 'rb') as file:\n",
        "                    pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "                    for page_num, page in enumerate(pdf_reader.pages):\n",
        "                        text = page.extract_text()\n",
        "\n",
        "                        if text and len(text.strip()) > 50:\n",
        "                            text = self._clean_extracted_text(text)\n",
        "                            page_chunks = self._smart_text_splitting(text, page_num + 1)\n",
        "                            chunks.extend(page_chunks)\n",
        "\n",
        "                print(f\"âœ… Texto extraÃ­do con PyPDF2: {len(chunks)} chunks\")\n",
        "\n",
        "            except Exception as e2:\n",
        "                print(f\"âŒ Error extrayendo PDF: {e2}\")\n",
        "                return []\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _clean_extracted_text(self, text: str) -> str:\n",
        "        \"\"\"Limpiar texto extraÃ­do del PDF\"\"\"\n",
        "        # Remover saltos de lÃ­nea excesivos\n",
        "        text = re.sub(r'\\n+', '\\n', text)\n",
        "        # Remover espacios mÃºltiples\n",
        "        text = re.sub(r' +', ' ', text)\n",
        "        # Remover caracteres extraÃ±os comunes en PDFs\n",
        "        text = re.sub(r'[^\\w\\s\\.\\,\\;\\:\\!\\?\\-\\(\\)Ã¡Ã©Ã­Ã³ÃºÃ±ÃÃ‰ÃÃ“ÃšÃ‘\\n]', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def _smart_text_splitting(self, text: str, page_num: int, chunk_size: int = 500) -> List[Dict]:\n",
        "        \"\"\"Dividir texto de manera inteligente\"\"\"\n",
        "        chunks = []\n",
        "\n",
        "        # Dividir por pÃ¡rrafos primero\n",
        "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "\n",
        "        current_chunk = \"\"\n",
        "        chunk_counter = 1\n",
        "\n",
        "        for paragraph in paragraphs:\n",
        "            # Si el pÃ¡rrafo cabe en el chunk actual\n",
        "            if len(current_chunk) + len(paragraph) < chunk_size:\n",
        "                current_chunk += paragraph + \"\\n\\n\"\n",
        "            else:\n",
        "                # Guardar chunk actual si no estÃ¡ vacÃ­o\n",
        "                if current_chunk.strip():\n",
        "                    chunks.append({\n",
        "                        'content': current_chunk.strip(),\n",
        "                        'page': page_num,\n",
        "                        'chunk': chunk_counter,\n",
        "                        'source': 'pdf',\n",
        "                        'type': 'text'\n",
        "                    })\n",
        "                    chunk_counter += 1\n",
        "\n",
        "                # Iniciar nuevo chunk\n",
        "                current_chunk = paragraph + \"\\n\\n\"\n",
        "\n",
        "        # Guardar Ãºltimo chunk\n",
        "        if current_chunk.strip():\n",
        "            chunks.append({\n",
        "                'content': current_chunk.strip(),\n",
        "                'page': page_num,\n",
        "                'chunk': chunk_counter,\n",
        "                'source': 'pdf',\n",
        "                'type': 'text'\n",
        "            })\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def index_pdf(self, pdf_path: str):\n",
        "        \"\"\"Indexar contenido del PDF\"\"\"\n",
        "        print(f\"ðŸ” Indexando PDF: {pdf_path}\")\n",
        "\n",
        "        # Extraer chunks del PDF\n",
        "        chunks = self.extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        if not chunks:\n",
        "            print(\"âŒ No se pudo extraer contenido del PDF\")\n",
        "            return False\n",
        "\n",
        "        self.chunks = chunks\n",
        "\n",
        "        # Generar embeddings\n",
        "        print(\"ðŸ§  Generando embeddings...\")\n",
        "        texts = [chunk['content'] for chunk in chunks]\n",
        "        embeddings = self.embedder.encode(texts, show_progress_bar=True)\n",
        "\n",
        "        if self.use_vector_db:\n",
        "            # ChromaDB\n",
        "            ids = [f\"chunk_{i}\" for i in range(len(chunks))]\n",
        "            metadatas = [{k: v for k, v in chunk.items() if k != 'content'} for chunk in chunks]\n",
        "\n",
        "            self.collection.add(\n",
        "                embeddings=embeddings.tolist(),\n",
        "                documents=texts,\n",
        "                metadatas=metadatas,\n",
        "                ids=ids\n",
        "            )\n",
        "            print(f\"âœ… {len(chunks)} chunks indexados en ChromaDB\")\n",
        "        else:\n",
        "            # FAISS\n",
        "            dimension = embeddings.shape[1]\n",
        "            self.index = faiss.IndexFlatL2(dimension)\n",
        "            self.index.add(embeddings.astype('float32'))\n",
        "            self.metadata = chunks\n",
        "            print(f\"âœ… {len(chunks)} chunks indexados en FAISS\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def search(self, query: str, k: int = 3) -> List[Dict]:\n",
        "        \"\"\"Buscar chunks relevantes\"\"\"\n",
        "        if not self.chunks:\n",
        "            print(\"âš ï¸ No hay contenido indexado\")\n",
        "            return []\n",
        "\n",
        "        # Generar embedding de la consulta\n",
        "        query_embedding = self.embedder.encode([query])\n",
        "\n",
        "        if self.use_vector_db:\n",
        "            # ChromaDB\n",
        "            results = self.collection.query(\n",
        "                query_embeddings=query_embedding.tolist(),\n",
        "                n_results=min(k, len(self.chunks))\n",
        "            )\n",
        "\n",
        "            search_results = []\n",
        "            for i in range(len(results['documents'][0])):\n",
        "                search_results.append({\n",
        "                    'content': results['documents'][0][i],\n",
        "                    'metadata': results['metadatas'][0][i],\n",
        "                    'score': results['distances'][0][i]\n",
        "                })\n",
        "        else:\n",
        "            # FAISS\n",
        "            scores, indices = self.index.search(query_embedding.astype('float32'), min(k, len(self.chunks)))\n",
        "\n",
        "            search_results = []\n",
        "            for i in range(len(indices[0])):\n",
        "                if indices[0][i] < len(self.chunks):\n",
        "                    search_results.append({\n",
        "                        'content': self.chunks[indices[0][i]]['content'],\n",
        "                        'metadata': self.chunks[indices[0][i]],\n",
        "                        'score': float(scores[0][i])\n",
        "                    })\n",
        "\n",
        "        return search_results\n",
        "\n",
        "    def generate_answer(self, query: str, search_results: List[Dict]) -> str:\n",
        "        \"\"\"Generar respuesta basada en el contexto\"\"\"\n",
        "        if not search_results:\n",
        "            return \"No se encontrÃ³ informaciÃ³n relevante en el PDF para responder la pregunta.\"\n",
        "\n",
        "        # Construir contexto\n",
        "        context_parts = []\n",
        "        sources = []\n",
        "\n",
        "        for result in search_results[:3]:  # Top 3 resultados\n",
        "            content = result['content']\n",
        "            metadata = result['metadata']\n",
        "\n",
        "            context_parts.append(content)\n",
        "            sources.append(f\"(PÃ¡gina {metadata['page']}, Chunk {metadata['chunk']})\")\n",
        "\n",
        "        context = \" \".join(context_parts)\n",
        "\n",
        "        # AnÃ¡lisis simple de la consulta para generar respuesta\n",
        "        query_lower = query.lower()\n",
        "        context_lower = context.lower()\n",
        "\n",
        "        # Buscar respuestas especÃ­ficas en el contexto\n",
        "        sentences = [s.strip() + '.' for s in context.split('.') if s.strip()]\n",
        "        relevant_sentences = []\n",
        "\n",
        "        # Palabras clave de la consulta\n",
        "        query_words = [word for word in query_lower.split() if len(word) > 3]\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_lower = sentence.lower()\n",
        "            # Contar coincidencias de palabras clave\n",
        "            matches = sum(1 for word in query_words if word in sentence_lower)\n",
        "            if matches > 0:\n",
        "                relevant_sentences.append((sentence, matches))\n",
        "\n",
        "        # Ordenar por relevancia\n",
        "        relevant_sentences.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        if relevant_sentences:\n",
        "            # Tomar las 2-3 oraciones mÃ¡s relevantes\n",
        "            answer_parts = [sent[0] for sent in relevant_sentences[:3]]\n",
        "            answer = \" \".join(answer_parts)\n",
        "        else:\n",
        "            # Respuesta por defecto con las primeras oraciones del contexto\n",
        "            answer = \". \".join(sentences[:2]) + \".\"\n",
        "\n",
        "        # Agregar fuentes\n",
        "        answer += f\" Fuentes: {', '.join(sources[:2])}\"\n",
        "\n",
        "        return answer\n",
        "\n",
        "    def query(self, question: str, k: int = 3, verbose: bool = True) -> Dict:\n",
        "        \"\"\"Consulta RAG completa\"\"\"\n",
        "        if verbose:\n",
        "            print(f\"\\nâ“ Pregunta: {question}\")\n",
        "\n",
        "        # Buscar contenido relevante\n",
        "        search_results = self.search(question, k)\n",
        "\n",
        "        if verbose and search_results:\n",
        "            print(f\"ðŸ“š Encontrados {len(search_results)} chunks relevantes:\")\n",
        "            for i, result in enumerate(search_results[:2]):\n",
        "                meta = result['metadata']\n",
        "                print(f\"  {i+1}. PÃ¡gina {meta['page']}, Chunk {meta['chunk']} (Score: {result['score']:.3f})\")\n",
        "\n",
        "        # Generar respuesta\n",
        "        answer = self.generate_answer(question, search_results)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"ðŸ’¡ Respuesta: {answer}\")\n",
        "\n",
        "        return {\n",
        "            'question': question,\n",
        "            'answer': answer,\n",
        "            'sources': search_results,\n",
        "            'context': \" \".join([r['content'] for r in search_results[:2]])\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# FUNCIÃ“N RÃPIDA PARA USAR CON EL PDF GENERADO\n",
        "# ============================================================================\n",
        "\n",
        "def quick_pdf_rag(pdf_path: str = \"demo_content.pdf\"):\n",
        "    \"\"\"FunciÃ³n rÃ¡pida para crear RAG con el PDF\"\"\"\n",
        "    print(\"ðŸš€ Configurando RAG rÃ¡pido para PDF...\")\n",
        "\n",
        "    # Verificar que el PDF existe\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(f\"âŒ PDF no encontrado: {pdf_path}\")\n",
        "        print(\"ðŸ“‚ PDFs disponibles:\")\n",
        "        pdfs = [f for f in os.listdir('.') if f.endswith('.pdf')]\n",
        "        for pdf in pdfs:\n",
        "            print(f\"   ðŸ“„ {pdf}\")\n",
        "        return None\n",
        "\n",
        "    # Crear sistema RAG\n",
        "    rag = PDFRAGSystem(use_vector_db=True)\n",
        "\n",
        "    # Indexar PDF\n",
        "    success = rag.index_pdf(pdf_path)\n",
        "\n",
        "    if not success:\n",
        "        print(\"âŒ Error indexando PDF\")\n",
        "        return None\n",
        "\n",
        "    print(\"âœ… RAG listo para consultas!\")\n",
        "    return rag"
      ],
      "metadata": {
        "id": "EUZV_qKwgifq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DEMO AUTOMÃTICO\n",
        "# ============================================================================\n",
        "\n",
        "print(\"ðŸ§ª INICIANDO DEMO RAG CON PDF\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Crear RAG con el PDF generado\n",
        "pdf_rag = quick_pdf_rag(\"demo_content.pdf\")\n",
        "\n",
        "if pdf_rag:\n",
        "    # Preguntas de prueba\n",
        "    test_questions = [\n",
        "        \"Â¿QuÃ© es la inteligencia artificial?\",\n",
        "        \"Â¿CÃ³mo funciona el aprendizaje automÃ¡tico?\",\n",
        "        \"Â¿QuÃ© son las redes neuronales?\",\n",
        "        \"Â¿CuÃ¡les son las aplicaciones de la IA?\"\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nðŸ§ª PROBANDO {len(test_questions)} PREGUNTAS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for i, question in enumerate(test_questions, 1):\n",
        "        print(f\"\\n--- Pregunta {i} ---\")\n",
        "        result = pdf_rag.query(question)\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    print(\"\\nðŸŽ‰ Demo completado!\")\n",
        "    print(\"\\nðŸ’¡ Para usar:\")\n",
        "    print(\"result = pdf_rag.query('tu pregunta aquÃ­')\")\n",
        "    print(\"print(result['answer'])\")\n",
        "else:\n",
        "    print(\"âŒ No se pudo configurar el RAG. Verifica que el PDF existe.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "53ededcb128a42cd9c8dd02ed67385a7",
            "a206fcef7de74de28207b0ed8ff70685",
            "6fc0e415bb04436aa49af026e859d18d",
            "3028cabdd9fd4258a956620b79613c7e",
            "2fab19445ef94af799ef6ad0eb67a805",
            "c94356b7de1a4cdaaebe4133751e879a",
            "33a3cf79a0a544be85798777ce8edc5e",
            "4a746c9e41494113b3d53d86d950d8c9",
            "517ed125753142e8a8e1b316712d8393",
            "c21c192e55914adbb8e7216a9c99b836",
            "ceca7e8853aa446ca79441e4f014239e"
          ]
        },
        "id": "3XuP8SljgwgK",
        "outputId": "a9acad30-4da1-42b2-b34d-0b5b28565495"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª INICIANDO DEMO RAG CON PDF\n",
            "==================================================\n",
            "ðŸš€ Configurando RAG rÃ¡pido para PDF...\n",
            "ðŸš€ Inicializando Sistema RAG para PDF...\n",
            "âœ… Modelo de embeddings cargado\n",
            "ðŸ“Š Configurando ChromaDB...\n",
            "âœ… Sistema RAG listo!\n",
            "ðŸ” Indexando PDF: demo_content.pdf\n",
            "ðŸ“„ Extrayendo texto de: demo_content.pdf\n",
            "âœ… Texto extraÃ­do: 7 chunks de 7 pÃ¡ginas\n",
            "ðŸ§  Generando embeddings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53ededcb128a42cd9c8dd02ed67385a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… 7 chunks indexados en ChromaDB\n",
            "âœ… RAG listo para consultas!\n",
            "\n",
            "ðŸ§ª PROBANDO 4 PREGUNTAS\n",
            "==================================================\n",
            "\n",
            "--- Pregunta 1 ---\n",
            "\n",
            "â“ Pregunta: Â¿QuÃ© es la inteligencia artificial?\n",
            "ðŸ“š Encontrados 3 chunks relevantes:\n",
            "  1. PÃ¡gina 2, Chunk 1 (Score: 0.346)\n",
            "  2. PÃ¡gina 1, Chunk 1 (Score: 0.437)\n",
            "ðŸ’¡ Respuesta: Inteligencia artificial - Wikipedia, la enciclopedia libre\n",
            "Fuente: https:  es. org wiki Inteligencia_artificial\n",
            "De Wikipedia, la enciclopedia libre Imagen generada por la inteligencia\n",
            "artificial Dalle 3. VÃ­deo explicativo de 6:47 min, en idioma euskera (con\n",
            "subtÃ­tulos en castellano) sobre la inteligencia artificial, incluyendo\n",
            "secciones sobre los dilemas Ã©ticos. Fuentes: (PÃ¡gina 2, Chunk 1), (PÃ¡gina 1, Chunk 1)\n",
            "----------------------------------------\n",
            "\n",
            "--- Pregunta 2 ---\n",
            "\n",
            "â“ Pregunta: Â¿CÃ³mo funciona el aprendizaje automÃ¡tico?\n",
            "ðŸ“š Encontrados 3 chunks relevantes:\n",
            "  1. PÃ¡gina 4, Chunk 1 (Score: 0.306)\n",
            "  2. PÃ¡gina 5, Chunk 1 (Score: 0.357)\n",
            "ðŸ’¡ Respuesta: Aprendizaje automÃ¡tico - Wikipedia, la enciclopedia libre\n",
            "Fuente: https:  es. org wiki Aprendizaje_automÃ¡tico\n",
            "De Wikipedia, la enciclopedia libre El aprendizaje automÃ¡tico (AA); tambiÃ©n\n",
            "llamado automatizado, computacional de mÃ¡quinas, o maquinal1 (del inglÃ©s\n",
            "machine learning, ML), es el subcampo de las ciencias de la computaciÃ³n y una\n",
            "rama de la inteligencia artificial, cuyo objetivo es desarrollar tÃ©cnicas que\n",
            "permitan que las computadoras aprendan. 2\n",
            "En el aprendizaje de mÃ¡quinas un computador observa datos, construye un modelo\n",
            "basado en esos datos y utiliza ese modelo a la vez como una hipÃ³tesis acerca\n",
            "del mundo y una pieza de software que puede resolver problemas. Fuentes: (PÃ¡gina 4, Chunk 1), (PÃ¡gina 5, Chunk 1)\n",
            "----------------------------------------\n",
            "\n",
            "--- Pregunta 3 ---\n",
            "\n",
            "â“ Pregunta: Â¿QuÃ© son las redes neuronales?\n",
            "ðŸ“š Encontrados 3 chunks relevantes:\n",
            "  1. PÃ¡gina 7, Chunk 1 (Score: 0.498)\n",
            "  2. PÃ¡gina 6, Chunk 1 (Score: 0.507)\n",
            "ðŸ’¡ Respuesta: Las redes neuronales actuales suelen contener desde unos miles a\n",
            "unos pocos millones de unidades neuronales. Nuevas investigaciones sobre el\n",
            "cerebro a menudo estimulan la creaciÃ³n de nuevos patrones en las redes\n",
            "neuronalescita requerida. Las redes neuronales se han utilizado para resolver una\n",
            "amplia variedad de tareas, como la visiÃ³n por computador y el reconocimiento de\n",
            "voz, que son difÃ­ciles de resolver usando la ordinaria programaciÃ³n basada en\n",
            "reglas. Fuentes: (PÃ¡gina 7, Chunk 1), (PÃ¡gina 6, Chunk 1)\n",
            "----------------------------------------\n",
            "\n",
            "--- Pregunta 4 ---\n",
            "\n",
            "â“ Pregunta: Â¿CuÃ¡les son las aplicaciones de la IA?\n",
            "ðŸ“š Encontrados 3 chunks relevantes:\n",
            "  1. PÃ¡gina 2, Chunk 1 (Score: 0.527)\n",
            "  2. PÃ¡gina 4, Chunk 1 (Score: 0.566)\n",
            "ðŸ’¡ Respuesta: El\n",
            "aprendizaje automÃ¡tico tiene una amplia gama de aplicaciones, incluyendo\n",
            "motores de bÃºsqueda, diagnÃ³sticos mÃ©dicos, detecciÃ³n de fraude en el uso de\n",
            "tarjetas de crÃ©dito, anÃ¡lisis de mercado para los diferentes sectores de\n",
            "actividad, clasificaciÃ³n de secuencias de ADN, reconocimiento del habla y del\n",
            "lenguaje escrito, juegos y robÃ³tica. Fuentes: (PÃ¡gina 2, Chunk 1), (PÃ¡gina 4, Chunk 1)\n",
            "----------------------------------------\n",
            "\n",
            "ðŸŽ‰ Demo completado!\n",
            "\n",
            "ðŸ’¡ Para usar:\n",
            "result = pdf_rag.query('tu pregunta aquÃ­')\n",
            "print(result['answer'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dDk7fc9yg6OK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}